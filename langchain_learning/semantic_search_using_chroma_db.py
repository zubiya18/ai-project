import os
from dotenv import load_dotenv
from bson.json_util import dumps  # âœ… Proper JSON serializer for MongoDB
from pymongo import MongoClient
from langchain_mistralai import MistralAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_community.vectorstores import Chroma

# Load environment variables
load_dotenv()

MONGO_URI = os.getenv("MONGO_URI")
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")

# Main function that handles all vector search operations
def vector_search_app():
    # Validate environment variables
    if not MISTRAL_API_KEY:
        print("âŒ MISTRAL_API_KEY not found in environment. Please check your .env file.")

    if not MONGO_URI:
        print("âŒ MONGO_URI not found in environment. Please check your .env file.")

    # Initialize components
    embeddings = MistralAIEmbeddings(model="mistral-embed")
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

    # Initialize Chroma collection
    collection = Chroma(
        collection_name="blog_vector_db",
        embedding_function=embeddings,
        persist_directory="chroma_storage"
    )

#mongo db connection to retrieve all documents of blog
    try:
            print("ğŸ”„ Connecting to MongoDB...")
            client = MongoClient(MONGO_URI)
            mongo_collection = client["app-dev"]["blogs"]
            blog_docs = list(mongo_collection.find({}))
            print(f"ğŸ“„ Found {len(blog_docs)} blog documents.")

            if not blog_docs:
                print("âŒ No documents retrieved from MongoDB.")
                return False

            # Prepare documents
            documents = []
            for blog in blog_docs:
                full_text = dumps(blog, indent=2)
                doc = Document(
                    page_content=full_text,

                )
                documents.append(doc)
            print(f"âœ… Prepared {len(documents)} documents for embedding.")

            # Split docs into chunks
            print("âœ‚ï¸ Splitting documents into chunks...")
            chunks = splitter.split_documents(documents)
            print(f"ğŸ”¨ Split into {len(chunks)} chunks.")

            # Limit chunks if needed
            chunk_limit = 100
            limited_chunks = chunks[:chunk_limit]
            print(f"ğŸ”¨ Using the first {len(limited_chunks)} chunks.")

            # Add documents to vector store
            print(f"ğŸš€ Adding {len(limited_chunks)} chunks to Chroma...")
            collection.add_documents(limited_chunks)
            collection.persist()
            print(f"âœ… Successfully stored {len(limited_chunks)} chunks in Chroma.")

    except Exception as e:
            print(f"âŒ Error building vector database: {e}")
            return False


    print("\nğŸ” Vector database ready for searching!")

    while True:
        query = input("\nğŸ” Enter search query (or type 'exit'): ")
        if query.lower() == "exit":
            print("ğŸšª Exiting the semantic search...")
            break

        # Perform search
        try:
            print(f"ğŸ” Performing semantic search for query: '{query}'...")

            # Get results
            results = collection.similarity_search(query, k=1)

            if results:
                print(f"\nğŸ“ Found {len(results)} relevant documents:")
                for doc in results:
                  print(doc.page_content)

            else:
                print("âŒ No relevant results found.")

        except Exception as e:
            print(f"âŒ Error during search: {e}")

    return True

# Main execution
if __name__ == "__main__":
    vector_search_app()
